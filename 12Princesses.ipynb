{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 Princesses Text Analysis\n",
    "\n",
    "From this link - download the 12dancingprincesses.txt file. Then read the file and use the NLTK library to tokenize each word in the text. This module uses the Natural Language Toolkit Library (NLTK) to look at individual words and sentences in a text and clean unneccessary features from the text data to prepare for sentiment analysis. Also using the textblob library, analysis on the sentiment of opinioned data will generate a numerical value for use in a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'datasets\\dancingprincesses.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing Words and Sentences\n",
    "\n",
    "Recall in the \"Python Dictionaries and String Manipulation\" notebook, we used the .split() function to break a sentence apart. However, because the default character to split on is a space, the .split() function does not work well with sentences that have punctuation.\n",
    "\n",
    "The NLTK library was built to separate punctuation from words when tokenizing (splitting into parts)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#this is sample data\n",
    "#from nltk.corpus import names  \n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "#if the next cell does not work\n",
    "#remove number symbol on following lines and re-run this cell\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('names')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array to hold the list of tokenized words\n",
    "OrigText = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in the text file and token each non-empty line\n",
    "\n",
    "with open(filepath, 'r') as f:\n",
    "    for line in f:\n",
    "        getline = line.strip() #get rid of newline character\n",
    "        #print(getline)\n",
    "        if getline == '': pass  #this will skip over lines that only had a newline and are now blank\n",
    "        else:\n",
    "            #first, change all the words to lowercase then Tokenize the words\n",
    "            getline = getline.lower()\n",
    "            txtTokens = word_tokenize(getline)\n",
    "            #print(txtTokens)\n",
    "            #Parse the words into the list\n",
    "            for word in txtTokens:\n",
    "                OrigText.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1853"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the length of the original text after the split function\n",
    "len(OrigText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefine a new set of punctuation with regards to the given text\n",
    "new_punctuation = punctuation + '’' + '‘'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the puntuation tokens from the list\n",
    "for word in OrigText:\n",
    "    if word in new_punctuation:\n",
    "        OrigText.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1618"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the length of the original text after removing all known punctuation\n",
    "len(OrigText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of english stopwords (filler words)\n",
    "eng_stopwords = stopwords.words('english')\n",
    "#eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_count = 0\n",
    "new_words = []  #list to hold new words\n",
    "\n",
    "for word in OrigText:\n",
    "    if word not in eng_stopwords:\n",
    "        new_words.append(word)\n",
    "    else: rm_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "673"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the length of the original text after removing all English filler words\n",
    "len(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soldier', 19),\n",
       " ('princesses', 17),\n",
       " ('said', 16),\n",
       " ('king', 15),\n",
       " ('twelve', 11),\n",
       " ('went', 11),\n",
       " ('came', 10),\n",
       " ('eldest', 10),\n",
       " ('one', 7),\n",
       " ('night', 7)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the NLTK FreqDist library function gives a count for how often each part of the text occurs ...in this case only 10 words\n",
    "fd_nw = FreqDist(new_words)\n",
    "fd_nw.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
